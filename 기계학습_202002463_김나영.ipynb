{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ8xPiWI2Ffi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset 불러오기\n",
        "\n",
        "train_data = np.loadtxt('/content/training.dat', unpack = True)\n",
        "test_data = np.loadtxt('/content/testing.dat', unpack = True)"
      ],
      "metadata": {
        "id": "cYJ3Y3zTBze2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test data 75개\n",
        "# 25개씩 나눠져서 총 3개의 클래스\n",
        "# 3개의 클래스 돌면서 라벨링 \n",
        "# one-hot encoding\n",
        "# >> 라벨값이 0 = [1, 0, 0] / 1 = [0, 1, 0] / 2 = [0, 0, 1]\n",
        "\n",
        "train_label = []\n",
        "test_label = []\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(25):\n",
        "        label = np.zeros(3)\n",
        "        label[i] = 1\n",
        "        train_label.append(label)\n",
        "        test_label.append(label)\n",
        "    \n",
        "train_label = np.array(train_label).T\n",
        "test_label = np.array(test_label).T"
      ],
      "metadata": {
        "id": "JTe9nmeSBvo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train의 shape / test의 shape\n",
        "\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "id": "efsDXyAqZqLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d94b55e-fc04-46ab-f3ec-26b1e81523de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 75)\n",
            "(4, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용할 활성화 함수 : ReLu\n",
        "# 마지막 layer에서 사용할 softmax 함수\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis = 0)"
      ],
      "metadata": {
        "id": "Zfx3E-geZZmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 정의\n",
        "# 파라미터 : X_train, y_train, learning_rate, epoch, activation function\n",
        "\n",
        "def model(X_train, y_train, learning_rate = 0.1, num_iterations = 20, activation = relu):\n",
        "\n",
        "    # 모델의 구조\n",
        "    # input : X.shape[0] = 4 / hidden layer는 100, 100 노드 \n",
        "    # output : softmax를 통해 3개의 클래스 중 분류하므로 3\n",
        "    layer_dims = [X_train.shape[0], 100, 100, 3]  \n",
        "\n",
        "    # grads -> 미분값 담기\n",
        "    grads = {}\n",
        "\n",
        "    train_costs = []\n",
        "    test_costs = []\n",
        "\n",
        "    # parameters -> 가중치값 담기\n",
        "    # 추후 두개의 딕셔너리를 통해 가중치에서 미분값을 연산해 가중치 조정\n",
        "    parameters = {}\n",
        "\n",
        "    # 가중치 초기화\n",
        "    # Glorot Initialization\n",
        "    for i in range(1, len(layer_dims)):\n",
        "        m = np.sqrt(6 / (layer_dims[i] + layer_dims[i - 1]))\n",
        "        parameters['W' + str(i)] = np.random.uniform(-m, m, size = (layer_dims[i], layer_dims[i - 1]))\n",
        "\n",
        "    # num_iteration\n",
        "    for i in range(0, num_iterations):  \n",
        "\n",
        "        # 순전파\n",
        "        # 가중치 W1, W2, W3\n",
        "        W1 = parameters[\"W1\"] #input -- hidden1\n",
        "        W2 = parameters[\"W2\"] # hidden1 -- hidden2\n",
        "        W3 = parameters[\"W3\"] # hidden2 -- output\n",
        "\n",
        "        # W1, input을 연산 (hidden1이 활성화 함수를 거치기 전)\n",
        "        Z1 = np.dot(W1, X_train)\n",
        "        # Z1이 활성화 함수를 거친 후 (hidden1의 출력값) \n",
        "        A1 = activation(Z1)\n",
        "\n",
        "        # W2, A1을 연산 (hidden2가 활성화 함수를 거치기 전)\n",
        "        Z2 = np.dot(W2, A1)\n",
        "        # Z2가 활성화 함수를 거친 후 (hidden2의 출력값) \n",
        "        A2 = activation(Z2)\n",
        "\n",
        "        # W3, A2을 연산 (최종 output이 활성화 함수를 거치기 전)\n",
        "        Z3 = np.dot(W3, A2)\n",
        "\n",
        "        # output에 softmax를 거쳐 총 3개의 값이 나옴\n",
        "        # 3개의 값이라고 할 때, 3개를 합치면 1 >> [0.1, 0.1, 0.8] 과 같이 모든 값을 합치면 1이 나옴\n",
        "        # 추후에 예측 과정에서 제일 높은 숫자를 가진 인덱스를 추출할 것 ([0.1, 0.1, 0.8] 이라면 0.8이 제일 높은 확률을 가지므로 예측 결과는 [0, 0, 1] 즉, 2가 됨)\n",
        "        y_pred = softmax(Z3)\n",
        "\n",
        "        # 비용함수\n",
        "        # Cross-Entropy\n",
        "        # softmax 함수를 거친 최종 output값이 자신의 라벨값과 비슷해질수록 낮아짐\n",
        "        # >>  실제 라벨이 [0, 0, 1] 이라면 / [0.1, 0.1, 0.8] 이 [0.2, 0.2, 0.6] 보다 낮은 loss값을 가짐\n",
        "        m = y_train.shape[1]\n",
        "        logprobs = np.multiply(-np.log(y_pred), y_train) + np.multiply(-np.log(1 - y_pred), 1 - y_train)\n",
        "        cost = 1./m * np.sum(logprobs)  \n",
        "\n",
        "\n",
        "        # 역전파\n",
        "        m = X_train.shape[1] \n",
        "        \n",
        "        # dZ3 는 최종 output과 실제값의 차이\n",
        "        # 각각의 dW 계산(- learning_rate * np.outer(전 계층_out, 위에서 내려온_delta))\n",
        "        dZ3 = y_pred - y_train\n",
        "        dW3 = 1./m * np.dot(dZ3, A2.T)\n",
        "\n",
        "        dA2 = np.dot(W3.T, dZ3)\n",
        "        dZ2 = np.multiply(dA2, np.where(np.dot(W2, A1) > 0, 1, 0))\n",
        "        dW2 = 1./m * np.dot(dZ2, A1.T)\n",
        "        \n",
        "        dA1 = np.dot(W2.T, dZ2)\n",
        "        dZ1 = np.multiply(dA1, np.where(np.dot(W1, X_train) > 0, 1, 0))\n",
        "        dW1 = 1./m * np.dot(dZ1, X_train.T)\n",
        "        \n",
        "        grads = {\"dZ3\": dZ3, \"dW3\": dW3, \n",
        "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \n",
        "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1,}\n",
        "\n",
        "        \n",
        "        # 가중치 업데이트\n",
        "        # grads : 미분값\n",
        "        # parameters : 가중치값\n",
        "        # W = W - lr * dW 식을 이용해서 가중치를 업데이트\n",
        "        for k in range(len(parameters) // 2):\n",
        "            parameters[\"W\" + str(k + 1)] = parameters[\"W\" + str(k + 1)] - learning_rate * grads[\"dW\" + str(k + 1)]\n",
        "        \n",
        "        # 최종적으로 구해진 오차값 저장\n",
        "        print(\"Cost after iteration {}: {}\".format(i, cost))\n",
        "        train_costs.append(cost)\n",
        "    \n",
        "    # 저장된 오차값 plot\n",
        "    # x축 : epoch / y축 : 오차값\n",
        "    plt.plot(train_costs, color = 'blue')\n",
        "    plt.ylabel('training_cost')\n",
        "    plt.xlabel('iterations')\n",
        "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    # 최종적으로 구해진 가중치값을 리턴해준다\n",
        "    return parameters     # 오차의 계산"
      ],
      "metadata": {
        "id": "oKXZnvTkZbWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = model(train_data, train_label, learning_rate = 0.05, num_iterations = 150, activation = relu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "un4_xzxv8pgD",
        "outputId": "2622f6ae-b8cf-437e-cf94-9656113811da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 1.820892024257944\n",
            "Cost after iteration 1: 1.7219184690979343\n",
            "Cost after iteration 2: 1.6651939931962532\n",
            "Cost after iteration 3: 1.6200989746419359\n",
            "Cost after iteration 4: 1.5776212240644485\n",
            "Cost after iteration 5: 1.5460226339272327\n",
            "Cost after iteration 6: 1.5227550694183534\n",
            "Cost after iteration 7: 1.5034939363051107\n",
            "Cost after iteration 8: 1.4861787138906062\n",
            "Cost after iteration 9: 1.4706125950504942\n",
            "Cost after iteration 10: 1.4564951610175214\n",
            "Cost after iteration 11: 1.443122775272619\n",
            "Cost after iteration 12: 1.4300514628205896\n",
            "Cost after iteration 13: 1.4182699789496584\n",
            "Cost after iteration 14: 1.406079262468106\n",
            "Cost after iteration 15: 1.39482153177749\n",
            "Cost after iteration 16: 1.382440205256037\n",
            "Cost after iteration 17: 1.3697834679527179\n",
            "Cost after iteration 18: 1.355062814962252\n",
            "Cost after iteration 19: 1.3395388820575054\n",
            "Cost after iteration 20: 1.3252799694201085\n",
            "Cost after iteration 21: 1.3120211665199297\n",
            "Cost after iteration 22: 1.3008337494307796\n",
            "Cost after iteration 23: 1.2921713307204377\n",
            "Cost after iteration 24: 1.284154060037711\n",
            "Cost after iteration 25: 1.2767122569586564\n",
            "Cost after iteration 26: 1.268943106596214\n",
            "Cost after iteration 27: 1.2603199987923048\n",
            "Cost after iteration 28: 1.2522403505696469\n",
            "Cost after iteration 29: 1.245661401775765\n",
            "Cost after iteration 30: 1.2393841099593907\n",
            "Cost after iteration 31: 1.2333804269656354\n",
            "Cost after iteration 32: 1.2276835110418722\n",
            "Cost after iteration 33: 1.222162477571853\n",
            "Cost after iteration 34: 1.2168064683062187\n",
            "Cost after iteration 35: 1.2114571755274544\n",
            "Cost after iteration 36: 1.2063384397931647\n",
            "Cost after iteration 37: 1.2012033513785343\n",
            "Cost after iteration 38: 1.1963044320928753\n",
            "Cost after iteration 39: 1.1915152487391603\n",
            "Cost after iteration 40: 1.1866606399544806\n",
            "Cost after iteration 41: 1.1818446393888842\n",
            "Cost after iteration 42: 1.1773648444937477\n",
            "Cost after iteration 43: 1.1727428322061084\n",
            "Cost after iteration 44: 1.1685430605325926\n",
            "Cost after iteration 45: 1.1638923652755944\n",
            "Cost after iteration 46: 1.159832399552124\n",
            "Cost after iteration 47: 1.1553799400518951\n",
            "Cost after iteration 48: 1.151408837541297\n",
            "Cost after iteration 49: 1.1470551590642266\n",
            "Cost after iteration 50: 1.1432317025001735\n",
            "Cost after iteration 51: 1.1388895157666816\n",
            "Cost after iteration 52: 1.1355191042104564\n",
            "Cost after iteration 53: 1.13094444357327\n",
            "Cost after iteration 54: 1.1276047235012425\n",
            "Cost after iteration 55: 1.1231222064448925\n",
            "Cost after iteration 56: 1.119861703299996\n",
            "Cost after iteration 57: 1.1154327801763126\n",
            "Cost after iteration 58: 1.1124578964163725\n",
            "Cost after iteration 59: 1.1080287576450405\n",
            "Cost after iteration 60: 1.1050458778606866\n",
            "Cost after iteration 61: 1.1006131673154185\n",
            "Cost after iteration 62: 1.0974648850427189\n",
            "Cost after iteration 63: 1.0934444662442622\n",
            "Cost after iteration 64: 1.0902428205704504\n",
            "Cost after iteration 65: 1.086286304897659\n",
            "Cost after iteration 66: 1.0831963047695787\n",
            "Cost after iteration 67: 1.0793230186985872\n",
            "Cost after iteration 68: 1.0762665154616464\n",
            "Cost after iteration 69: 1.0724111078748566\n",
            "Cost after iteration 70: 1.0695214780627984\n",
            "Cost after iteration 71: 1.0657297131628818\n",
            "Cost after iteration 72: 1.0629434632043302\n",
            "Cost after iteration 73: 1.0591839272497714\n",
            "Cost after iteration 74: 1.0563101527676617\n",
            "Cost after iteration 75: 1.052911025327818\n",
            "Cost after iteration 76: 1.0501310309479626\n",
            "Cost after iteration 77: 1.0467039523237114\n",
            "Cost after iteration 78: 1.0437664910122646\n",
            "Cost after iteration 79: 1.0404441110116147\n",
            "Cost after iteration 80: 1.0374907240124467\n",
            "Cost after iteration 81: 1.034338149757734\n",
            "Cost after iteration 82: 1.031371704112336\n",
            "Cost after iteration 83: 1.0283650363567474\n",
            "Cost after iteration 84: 1.0256221118493842\n",
            "Cost after iteration 85: 1.022501685236422\n",
            "Cost after iteration 86: 1.0197721613437503\n",
            "Cost after iteration 87: 1.0167174924012663\n",
            "Cost after iteration 88: 1.0139807307313407\n",
            "Cost after iteration 89: 1.010833923438524\n",
            "Cost after iteration 90: 1.0079805338857992\n",
            "Cost after iteration 91: 1.0049532916484678\n",
            "Cost after iteration 92: 1.0021306206445484\n",
            "Cost after iteration 93: 0.999128554171931\n",
            "Cost after iteration 94: 0.9963520217665887\n",
            "Cost after iteration 95: 0.9935197045452873\n",
            "Cost after iteration 96: 0.9907733566106524\n",
            "Cost after iteration 97: 0.9879361445267664\n",
            "Cost after iteration 98: 0.9853186528065226\n",
            "Cost after iteration 99: 0.9826850589912111\n",
            "Cost after iteration 100: 0.9801635759200656\n",
            "Cost after iteration 101: 0.9775937735569498\n",
            "Cost after iteration 102: 0.9751076660168562\n",
            "Cost after iteration 103: 0.9726194452556688\n",
            "Cost after iteration 104: 0.9702701748425125\n",
            "Cost after iteration 105: 0.9677175310528299\n",
            "Cost after iteration 106: 0.9653479303751742\n",
            "Cost after iteration 107: 0.9628709213259591\n",
            "Cost after iteration 108: 0.9605796289257101\n",
            "Cost after iteration 109: 0.9581197856087088\n",
            "Cost after iteration 110: 0.9558612127089828\n",
            "Cost after iteration 111: 0.953439100519004\n",
            "Cost after iteration 112: 0.9512892388798856\n",
            "Cost after iteration 113: 0.9489025035924766\n",
            "Cost after iteration 114: 0.9467778319418385\n",
            "Cost after iteration 115: 0.9444446140848489\n",
            "Cost after iteration 116: 0.9422727838279215\n",
            "Cost after iteration 117: 0.9401068153653515\n",
            "Cost after iteration 118: 0.9378416073906867\n",
            "Cost after iteration 119: 0.9357230398983821\n",
            "Cost after iteration 120: 0.933536502436409\n",
            "Cost after iteration 121: 0.9315093728891287\n",
            "Cost after iteration 122: 0.9293228000584693\n",
            "Cost after iteration 123: 0.9272267378287814\n",
            "Cost after iteration 124: 0.9251076778196806\n",
            "Cost after iteration 125: 0.9231478240789813\n",
            "Cost after iteration 126: 0.9209526839123764\n",
            "Cost after iteration 127: 0.9190908134345791\n",
            "Cost after iteration 128: 0.9168992943024198\n",
            "Cost after iteration 129: 0.914966905950856\n",
            "Cost after iteration 130: 0.9128666048312247\n",
            "Cost after iteration 131: 0.9109994096008281\n",
            "Cost after iteration 132: 0.9089264181182279\n",
            "Cost after iteration 133: 0.9070323711063877\n",
            "Cost after iteration 134: 0.9049957436105286\n",
            "Cost after iteration 135: 0.9031182546925288\n",
            "Cost after iteration 136: 0.9011067167279533\n",
            "Cost after iteration 137: 0.8992397341780847\n",
            "Cost after iteration 138: 0.8972320831401749\n",
            "Cost after iteration 139: 0.8953151445875093\n",
            "Cost after iteration 140: 0.8933299050587176\n",
            "Cost after iteration 141: 0.8914195208388594\n",
            "Cost after iteration 142: 0.8894404869431077\n",
            "Cost after iteration 143: 0.887499344169495\n",
            "Cost after iteration 144: 0.8855493715940179\n",
            "Cost after iteration 145: 0.8835770168439344\n",
            "Cost after iteration 146: 0.8816951995142086\n",
            "Cost after iteration 147: 0.8797430603218787\n",
            "Cost after iteration 148: 0.8778639708148558\n",
            "Cost after iteration 149: 0.8759209641584916\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 함수\n",
        "# 저장된 가중치값을 가지고 있기 때문에\n",
        "# test 데이터를 이 가중치값으로 순전파를 통과시켜주면 모델의 예측값이 나옴\n",
        "def predict(X_train, y_train, parameters, activation = relu):\n",
        "    W1 = parameters[\"W1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    W3 = parameters[\"W3\"]\n",
        "\n",
        "    # 위의 훈련시 사용한 순전파 모델\n",
        "    Z1 = np.dot(W1, X_train)\n",
        "    A1 = activation(Z1)\n",
        "    Z2 = np.dot(W2, A1)\n",
        "    A2 = activation(Z2)\n",
        "    Z3 = np.dot(W3, A2)\n",
        "    y_pred = softmax(Z3)\n",
        "\n",
        "    # y_pred 값은 총 3개의 데이터로 이뤄져 있기 때문에\n",
        "    # 그대로 사용 x ([0.1, 0.2, 0.7] 이런식으로 각 클래스에 대한 확률이 나와있음)\n",
        "    # 제일 큰 확률을 가진 클래스를 찾아주는 과정 필요 -> np.argmax()\n",
        "    prediction = np.argmax(y_pred, axis = 0)\n",
        "    return prediction\n",
        "\n",
        "# 이렇게 얻은 모델 가중치로 training data, test data를 각각 예측해본 결과\n",
        "# 정확도\n",
        "train_pred = predict(train_data, train_label, parameters)\n",
        "print (\"On the training set :\")\n",
        "print(\"Accuracy : \"  + str(np.mean((train_pred[:] == np.argmax(train_label, axis = 0)))))\n",
        "\n",
        "test_pred = predict(test_data, test_label, parameters)\n",
        "print (\"\\nOn the test set :\")\n",
        "print(\"Accuracy : \"  + str(np.mean((test_pred[:] == np.argmax(test_label, axis = 0)))))"
      ],
      "metadata": {
        "id": "7VFysHDZZdx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "058951f1-1ed0-40a1-831a-1809775ff9ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On the training set :\n",
            "Accuracy : 0.96\n",
            "\n",
            "On the test set :\n",
            "Accuracy : 0.96\n"
          ]
        }
      ]
    }
  ]
}